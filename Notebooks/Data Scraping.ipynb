{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aquisition\n",
    "\n",
    "We will use __PushShift__ to extract the data of subrredit _India_ from 7/01/2020 till 7/04/2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def getPushshiftData(after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission?&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "def collectSubData(subm):\n",
    "    subData = list() #list to store data points\n",
    "    title = subm['title']\n",
    "    url = subm['url']\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"    \n",
    "    author = subm['author']\n",
    "    sub_id = subm['id']\n",
    "    score = subm['score']\n",
    "    created = datetime.datetime.fromtimestamp(subm['created_utc']) \n",
    "    numComms = subm['num_comments']\n",
    "    permalink = subm['permalink']\n",
    "    try:\n",
    "        selftext=subm['selftext']\n",
    "    except KeyError:\n",
    "        selftext=\"NaN\"\n",
    "    over_18=subm['over_18']\n",
    "    \n",
    "    \n",
    "    subData.append((sub_id,title,url,author,score,created,numComms,permalink,flair,over_18,selftext))\n",
    "    subStats[sub_id] = subData\n",
    "    \n",
    "#Subreddit to query.\n",
    "sub=\"india\"\n",
    "#before and after dates\n",
    "before = \"1586284088\" #April 7st\n",
    "after = \"1577895896\"  #January 7st \n",
    "\n",
    "subCount = 0\n",
    "subStats = {}\n",
    "\n",
    "# Will run until all posts have been gathered \n",
    "# from the 'after' date up until before date\n",
    "while len(data) > 0:\n",
    "    for submission in data:\n",
    "        collectSubData(submission)\n",
    "        subCount+=1\n",
    "        \n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    \n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    after = data[-1]['created_utc']\n",
    "    data = getPushshiftData(after, before, sub)\n",
    "    \n",
    "# Storing data in csv file\n",
    "\n",
    "def updateSubs_file():\n",
    "    upload_count = 0\n",
    "    \n",
    "    file = \"post_data.csv\" #filename\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
    "        a = csv.writer(file, delimiter=',')\n",
    "        headers = [\"Post ID\",\"Title\",\"Url\",\"Author\",\"Score\",\"Publish Date\",\"Total No. of Comments\",\"Permalink\",\"Flair\",\"Over 18\",\"Selftext\"]\n",
    "        a.writerow(headers)\n",
    "        for sub in subStats:\n",
    "            a.writerow(subStats[sub][0])\n",
    "            upload_count+=1\n",
    "            \n",
    "        print(str(upload_count) + \" submissions have been uploaded\")\n",
    "updateSubs_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44027, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('post_data.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Comments using praw and adding it to the already scraped Pushshift dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Enter your credentials here\n",
    "reddit=praw.Reddit(client_id='xxxx',\n",
    "client_secret='xxxxx',\n",
    "user_agent='xxxx',\n",
    "password='xxxx')\n",
    "\n",
    "# Read the data extracted from Pushshitft\n",
    "data=pd.read_csv('post_data.csv')\n",
    "\n",
    "# Add an empty column \"Comments\"\n",
    "data['Comments']=\"\"\n",
    "\n",
    "n=len(data)\n",
    "coms=[]\n",
    "\n",
    "def getComments(id):\n",
    "    \n",
    "    #url of the reddit post\n",
    "    url=\"https://www.reddit.com/r/india/comments/\"+id\n",
    "    \n",
    "    submission=reddit.submission(url=url)\n",
    "    \n",
    "    #escape \"Load more comments\"\n",
    "    submission.comments.replace_more(limit=0)\n",
    "    comments=submission.comments.list()\n",
    "    \n",
    "    count=1\n",
    " \n",
    "    coms.clear()\n",
    "    \n",
    "    for com in comments:\n",
    "        \n",
    "        if count<11:   #getting top 10 comments\n",
    "            coms.append(com.body)\n",
    "            count+=1\n",
    "        else:\n",
    "            break  \n",
    "          \n",
    "    return list(coms) #coms contains 10 comments of a post\n",
    "        \n",
    "for i in range (0,n):\n",
    "    #get the post_id from the dataset\n",
    "    id=data['Post ID'][i]\n",
    "    data.at[i,'Comments']=getComments(id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the dataset for further operations.\n",
    "data=pd.to_csv('data_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
